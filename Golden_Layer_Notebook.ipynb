{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b82ffb9-d599-4843-a5b9-a90a36ed8fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# This Notebook is Made to Create Golden Layer Tables from Silver Layer Tables as Tables to be Used in Star Schema or Aggregated Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73896962-aadd-4135-9bbe-50e421ed3fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Packages and Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db899c6-85bf-467f-af4f-badeb6e99219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "# import pyspark as spark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import countDistinct,md5,concat,when,col\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,StringType,VarcharType,LongType,DecimalType,FloatType,StructField,StructType\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "import dlt\n",
    "from pyspark.sql import functions as F, Window\n",
    "import csv \n",
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135ce890-0ff4-4b5d-8c56-6b245bf07278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting Variables and Dataframes from config notebook\n",
    "%run /Workspace/Users/cihangiray.oner@gmail.com/congif.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1353903e-9cb8-4d6d-a6ee-bd40ee692d99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating Day Parameter for General KPI`S calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d399b272-106b-4c1c-b194-6ac3ea71773f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "day_parameter = dbutils.widgets.dropdown(\"day_parameter\", \"2\", [\"2\", \"8\", \"15\", \"31\"])\n",
    "\n",
    "day_parameter = dbutils.widgets.get(\"day_parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddd1717-9dd8-43ff-81a6-8bd6fc2d364a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading Tables from Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0a05d3-16a0-46e6-a926-ba83fc2134aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_if_table_exists(read_table_name,read_schema_name):\n",
    "    \"\"\"\n",
    "    Checks if a table exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return spark.catalog.tableExists(f\"hive_metastore.{read_schema_name}.{read_table_name}\")\n",
    "\n",
    "\n",
    "def read_table(dataframe_name,schema_name,table_name,columns_to_read,filter_value = \"N/A\",fill_value = \"N/A\"):\n",
    "    \"\"\"\n",
    "    Reads table and assign it to a dataframe from databricks schemas.\n",
    "\n",
    "    Args:\n",
    "        schema_name (str): The name of the schema to read table from.\n",
    "        table_name (str): The name of the table to read data from.\n",
    "        columns_to_read (list): List containing column names from table to be read\n",
    "        fill_value (str): String value to be used in null value conversion\n",
    "        filter_value (str): Filter parameter for reading table with filtering by spesific values\n",
    "\n",
    "    Returns:\n",
    "        spark.DataFrame: DataFrame after reading table gets into DataFrame.\n",
    "    \"\"\"\n",
    "    #check if table exists\n",
    "    if check_if_table_exists(table_name,schema_name):\n",
    "        dataframe_name = spark.table(f\"hive_metastore.{schema_name}.{table_name}\").select(columns_to_read).na.fill(f'{fill_value}')\n",
    "    else:\n",
    "        print(f\"Read table not found hive_metastore.{schema_name}.{table_name}\")\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2e3b90-f790-4f94-93cc-4f102f9cb829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_app_purchase_df_silver_layer_managed_table\nlogin_df_silver_layer_managed_table\nmultiplayer_battle_df_silver_layer_managed_table\nnew_user_df_silver_layer_managed_table\nsession_started_df_silver_layer_managed_table\nship_transaction_df_silver_layer_managed_table\nuser_id_df_silver_layer_managed_table\n"
     ]
    }
   ],
   "source": [
    "for table_name in golden_layer_read_table_names_list:\n",
    "    print(table_name)\n",
    "    # Creating dataframes by create_dataframe function\n",
    "    vars() [table_name] = read_table(table_name,second_layer_schema_name,f\"{table_name}\",\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8189c5cd-e096-46ad-960c-52e8ce098461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Renaming Dim Tables and Create & Rename Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683e7557-444f-4d32-9899-a6403876496e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "golden_layer_table_names = []\n",
    "for df_name in golden_layer_read_table_names_list:\n",
    "    new_table_name = \"\".join((\"d_\", df_name)).replace(\"_df_silver_layer_managed_table\", \"\")\n",
    "    # Appending new table names into a list for further usage\n",
    "    golden_layer_table_names.append(new_table_name)\n",
    "    vars()[new_table_name] = globals()[df_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6834f1-a8f2-4ff6-9a19-66829eeb661e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Rename Columns of Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436b58b4-794e-4f39-85f9-9c810dec7227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns(dataframe_name,columns_to_rename):\n",
    "    \"\"\"\n",
    "\t    Renames the columns of a PySpark DataFrame based on the provided configuration. All Notebook names are defined in namespace of related config file section\n",
    "\t    e.g. F_MES_ACTIVITIES_COLUMN_RENAME:\n",
    "                PLANT:PLANT>\n",
    "                DESCRIPTION:OPERATION>\n",
    "\t    Args:\n",
    "\t        DataFrame (pyspark.sql.DataFrame): The DataFrame to rename columns.\n",
    "\t    Returns:\n",
    "\t        pyspark.sql.DataFrame: The DataFrame with renamed columns.\n",
    "\t    Example:\n",
    "\t        >>> dataframe = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], [\"id\", \"name\"])\n",
    "\t        >>> rename_columns_with_config_file(dataframe, config)\n",
    "\t        DataFrame[user_id: int, user_name: string]\n",
    "        \"\"\"\n",
    "    counter_rename = 0\n",
    "    while counter_rename < len(columns_to_rename)-1:\n",
    "        # global Material_master_df\n",
    "        dataframe_name = dataframe_name.withColumnRenamed((columns_to_rename[counter_rename]),(columns_to_rename[counter_rename+1]))\n",
    "        counter_rename += 2\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4d632b-bc95-48b3-b694-c7c71f4dbc62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for df_name,columns_to_rename in golden_layer_col_names.items():\n",
    "    globals()[df_name] = rename_columns(globals()[df_name],columns_to_rename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f900ad6-0676-41af-9925-a1798854e699",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Based on Data Model Creating Fact Table with Joining (SHIP_TRANSACTION_LOG,SESSION_STARTED,MULTIPLAYER_BATTLE_STARTED,IN_APP_PURCHASE_LOG_SERVER) Tables on \"Join_Key\" column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8b7ce0-b700-4a97-8c5a-a87b853ea18a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f_multi_ships = d_session_started.join(d_multiplayer_battle,'Join_key',\"left\")\\\n",
    "                        .join(d_in_app_purchase,'Join_key', how=\"left\")\\\n",
    "                        .join(d_ship_transaction,'Join_key', how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "golden_layer_table_names.append('f_multi_ships')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1427ed-dea3-4748-8b78-a5bf85025240",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Writing Dataframes into Hive Metastore Golden Layer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04ca240-b2be-462f-af98-717e80045832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(third_layer_schema_name,location_name):\n",
    "    \"\"\"\n",
    "    Checks if a schema exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        location_name (str): catalog name to save schema on\n",
    "        third_layer_schema_name (str): The name of the schema to check.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DF creates schema on defined catalog if not exits\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"CREATE SCHEMA IF NOT EXISTS  {location_name}.{third_layer_schema_name}\")\n",
    "\n",
    "\n",
    "def check_if_table_exists(third_layer_schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Checks if a table exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return spark.catalog.tableExists(f\"hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "\n",
    "def write_to_managed_table(df, table_name, third_layer_schema_name, location_name,partition_cols = [], mode = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a managed table in Delta Lake.\n",
    "\n",
    "    If the table exists and mode is overwrite, it performs an overwrite operation.\n",
    "    Otherwise, it either creates a new table or appends transactions to table based on the `mode` parameter.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The DataFrame to write to the table.\n",
    "        table_name (str): The name of the target table.\n",
    "        third_layer_schema_name (str): The schema name of target table.\n",
    "        location_name (str): Catalog name to save schema on\n",
    "        mode (str, optional): The write mode.\n",
    "    \"\"\"\n",
    "    #create schema if not exists\n",
    "    create_schema(third_layer_schema_name,location_name)\n",
    "    # check if the table exists\n",
    "    if check_if_table_exists(third_layer_schema_name, table_name):\n",
    "        print(f\"Table exists on hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "        if mode == \"overwrite\":\n",
    "            print(f\"Overwriting all transactions on managed table hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "            df.write.format(\"delta\").partitionBy(partition_cols).option(\"overwriteSchema\", \"true\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "        else:\n",
    "            print(f\"Appending all transactions on managed table hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "            df.write.format(\"delta\").partitionBy(partition_cols).option(\"overwriteSchema\", \"true\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "    else:\n",
    "        print(f\"Writing to managed table hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")\n",
    "        df.write.format(\"delta\").partitionBy(partition_cols).option(\"overwriteSchema\", \"true\").option(\"delta.columnMapping.mode\", \"name\").saveAsTable(f\"hive_metastore.{third_layer_schema_name}.{table_name}_golden_layer_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0cdb0e0-54b6-4cf2-aa83-03e4007c9884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_in_app_purchase\nTable exists on hive_metastore.third_layer_pipeline.d_in_app_purchase_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_in_app_purchase_golden_layer_managed_table\nd_login\nd_multiplayer_battle\nTable exists on hive_metastore.third_layer_pipeline.d_multiplayer_battle_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_multiplayer_battle_golden_layer_managed_table\nd_new_user\nTable exists on hive_metastore.third_layer_pipeline.d_new_user_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_new_user_golden_layer_managed_table\nd_session_started\nTable exists on hive_metastore.third_layer_pipeline.d_session_started_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_session_started_golden_layer_managed_table\nd_ship_transaction\nTable exists on hive_metastore.third_layer_pipeline.d_ship_transaction_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_ship_transaction_golden_layer_managed_table\nd_user_id\nTable exists on hive_metastore.third_layer_pipeline.d_user_id_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.d_user_id_golden_layer_managed_table\nf_multi_ships\nTable exists on hive_metastore.third_layer_pipeline.f_multi_ships_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.f_multi_ships_golden_layer_managed_table\n"
     ]
    }
   ],
   "source": [
    "for df_name in golden_layer_table_names:\n",
    "    print(df_name)\n",
    "    # Writing dataframes to 3rd layer on defined schema and table (Except Login Table)\n",
    "    if df_name != \"d_login\" and df_name != \"f_multi_ships\":\n",
    "        write_to_managed_table(vars()[df_name], df_name,  third_layer_schema_name, location_name)\n",
    "    # Partitioning fact table on most used date column for data pruning in case direct query setup \n",
    "    elif df_name == \"f_multi_ships\":\n",
    "        write_to_managed_table(vars()[df_name], df_name,  third_layer_schema_name, location_name,[\"SESSION_EVENT_TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d32400-38b6-4e6c-84e8-cd767ebf8428",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requested Analytical Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad1cacb-8141-47ef-bd7e-77e873490732",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  General KPIs:\n",
    " - metrics: Active Users, New Users, Revenue, Spenders(Buyers), ARPU, ARPPU, 1 Day \n",
    "Retention rate, 3 Day Retention rate, 7 Day Retention rate, 7 day Conversion Rate\n",
    "\n",
    "- period: Daily, Monthly, Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6244ee8-166a-4997-a635-f4738e8ef1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Active Users (Prefiltering f_multi_ships dataframe by period defined on day_parameter)\n",
    "\n",
    "f_multi_ships_a_users = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval {day_parameter} days'))\n",
    "kpi_active_user_count = f_multi_ships_a_users.agg(countDistinct(col(\"SESSION_USER_ID\")).alias(\"Active_User_Number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b645b857-c619-4e8f-a1a9-1e28e9262a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New Users (Prefiltering d_new_users dataframe by period defined on day_parameter)\n",
    "\n",
    "d_new_user_n_users = d_new_user.filter(F.col('USER_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval {day_parameter} days'))\n",
    "kpi_new_user_count = d_new_user_n_users.agg(countDistinct(col(\"USER_USER_ID\")).alias(\"New_User_Number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6211f7-80df-4063-9572-9ef2be40cd8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revenue Sum (Prefiltering f_multi_ships dataframe by period defined on day_parameter)\n",
    "\n",
    "f_multi_ships_a_users = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval {day_parameter} days'))\n",
    "kpi_revenue_sum = f_multi_ships_a_users.agg(sum(col(\"IN_APP_USD_COST\")).cast('decimal(12,2)').alias(\"Total_Revenue\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e3d424-f7ba-4978-adc8-56599ee2177e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spender Users (This KPI will be saved and calculated for all times)\n",
    "\n",
    "kpi_spenders_count = d_user_id.filter(d_user_id.USER_IS_SPENDER == True).agg(count(col(\"USER_IS_SPENDER\")).alias(\"Spender_User_Number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d9a370-6c6c-46c5-bada-37b2defd5b50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arpu (Time period of this KPI will be calculated by day_parameter)\n",
    "\n",
    "kpi_arpu = [str((kpi_revenue_sum.first()['Total_Revenue'])/kpi_active_user_count.first()['Active_User_Number'])]\n",
    "rdd = sc.parallelize(kpi_arpu)\n",
    "rdd = rdd.map(lambda x:[x])\n",
    "schema = StructType([StructField(\"Arpu_Value\", StringType(), True)])\n",
    "kpi_arpu_df = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae34bb37-208b-4de0-96c0-e47c96007f63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arrpu (Time period of this KPI will be calculated by day_parameter)\n",
    "\n",
    "# kpi_arrpu = [str((kpi_revenue_sum.first()['Total_Revenue'])/kpi_spenders_count.first()['Spender_User_Number'])]\n",
    "kpi_arrpu = [str((kpi_revenue_sum.first()['Total_Revenue'])/1)]\n",
    "rdd = sc.parallelize(kpi_arrpu)\n",
    "rdd = rdd.map(lambda x:[x])\n",
    "schema = StructType([StructField(\"Arpu_Value\", StringType(), True)])\n",
    "kpi_arrpu_df = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beee9697-b973-4f74-bbfe-88f07a91ef31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 DAY RETENTION RATE (UNIQUE USER_ID Counts from fact table who plays a multi game  (Meaning active users in last n day session openers) // New User Number) (time period has taken back from prefiltered dataframe named as f_multi_ships_a_users  + 1 day)\n",
    "\n",
    "one_day_active_session_user = f_multi_ships_a_users.withColumn('maxdate',F.max('MULTIPLAYER_EVENT_TIMESTAMP').over(Window.orderBy(F.lit(1)))).filter('MULTIPLAYER_EVENT_TIMESTAMP >= maxdate - interval 1 days').drop('maxdate')\n",
    "one_day_active_session_user = one_day_active_session_user.agg(countDistinct(col(\"MULTIPLAYER_USER_ID\")).alias(\"Last_1_Day_Game_Players\"))                        \n",
    "kpi_one_day_ret_rate = [str((one_day_active_session_user.first()['Last_1_Day_Game_Players'])/(kpi_new_user_count.first()['New_User_Number']))]\n",
    "rdd = sc.parallelize(kpi_one_day_ret_rate)\n",
    "rdd = rdd.map(lambda x:[x])\n",
    "schema = StructType([StructField(\"kpi_one_day_ret_rate\", StringType(), True)])\n",
    "kpi_one_day_ret_rate = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfd721e-922a-40e5-9cce-61698c5690fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3 DAY RETENTION RATE (UNIQUE USER_ID Counts from fact table who plays a multi game (Meaning active users in last n day session openers) // New User Number) (time period has taken back from prefiltered dataframe named as f_multi_ships_a_users  + 3 day)\n",
    "\n",
    "three_day_active_session_user = f_multi_ships_a_users.withColumn('maxdate',F.max('MULTIPLAYER_EVENT_TIMESTAMP').over(Window.orderBy(F.lit(1)))).filter('MULTIPLAYER_EVENT_TIMESTAMP >= maxdate - interval 3 days').drop('maxdate')\n",
    "three_day_active_session_user = three_day_active_session_user.agg(countDistinct(col(\"MULTIPLAYER_USER_ID\")).alias(\"Last_3_Days_Game_Players\"))                         \n",
    "kpi_three_day_ret_rate = [str((three_day_active_session_user.first()['Last_3_Days_Game_Players'])/(kpi_new_user_count.first()['New_User_Number']))]\n",
    "rdd = sc.parallelize(kpi_three_day_ret_rate)\n",
    "rdd = rdd.map(lambda x:[x])\n",
    "schema = StructType([StructField(\"kpi_three_day_ret_rate\", StringType(), True)])\n",
    "kpi_three_day_ret_rate = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e52d2a7-2dc3-4ab0-842e-29398e341715",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7 DAY RETENTION RATE (UNIQUE USER_ID Counts from fact table who plays a multi game (Meaning active users in last n day session openers) // New User Number) (time period has taken back from prefiltered dataframe named as f_multi_ships_a_users  + 7 day)\n",
    "\n",
    "seven_day_active_session_user = f_multi_ships_a_users.withColumn('maxdate',F.max('MULTIPLAYER_EVENT_TIMESTAMP').over(Window.orderBy(F.lit(1)))).filter('MULTIPLAYER_EVENT_TIMESTAMP >= maxdate - interval 7 days').drop('maxdate')\n",
    "seven_day_active_session_user = seven_day_active_session_user.agg(countDistinct(col(\"MULTIPLAYER_USER_ID\")).alias(\"Last_7_Days_Game_Players\"))                   \n",
    "kpi_seven_day_ret_rate = [str((seven_day_active_session_user.first()['Last_7_Days_Game_Players'])/(kpi_new_user_count.first()['New_User_Number']))]\n",
    "rdd = sc.parallelize(kpi_seven_day_ret_rate)\n",
    "rdd = rdd.map(lambda x:[x])\n",
    "schema = StructType([StructField(\"kpi_seven_day_ret_rate\", StringType(), True)])\n",
    "kpi_seven_day_ret_rate = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9e1692-9e55-4689-89d9-cc2cf7c4bab8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7 DAY CONVERSION RATE (UNIQUE User ID Counts from fact table who purchases an item (Meaning Buyers in last n day ) // New User Number) (time period has taken back from prefiltered dataframe named as f_multi_ships_a_users  + 7 day)\n",
    "\n",
    "seven_day_buyer_user_count = f_multi_ships_a_users.withColumn('maxdate',F.max('IN_APP_EVENT_TIMESTAMP').over(Window.orderBy(F.lit(1)))).filter('IN_APP_EVENT_TIMESTAMP >= maxdate - interval 7 days').drop('maxdate')\n",
    "seven_day_buyer_user_count = seven_day_buyer_user_count.agg(countDistinct(col(\"IN_APP_USER_ID\")).alias(\"Last_7_Days_item_buyers\"))\n",
    "kpi_seven_day_conv_rate = [str((seven_day_buyer_user_count.first()['Last_7_Days_item_buyers'])/(kpi_new_user_count.first()['New_User_Number']))]\n",
    "rdd = sc.parallelize(kpi_seven_day_conv_rate)\n",
    "rdd = rdd.map(lambda x:[x]) \n",
    "schema = StructType([StructField(\"kpi_seven_day_conv_rate\", StringType(), True)])\n",
    "kpi_seven_day_conv_rate = spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85158d10-98dd-40e3-af04-a51979cfa723",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ships saturation:\n",
    " - ships owned by a every user every day\n",
    " - daily ships popularity\n",
    " (check comments for SHIP_TRANSACTION_LOG table for some additional details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e920341-3a5a-4a42-8b6b-77df6a0c8b15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ships owned by a every user every day\n",
    "\n",
    "kpi_ships_popularity = f_multi_ships.filter(F.col('SHIP_TRANS_SHIP_NAME') != 'null').groupBy(\"SESSION_USER_ID\",\"SHIP_TRANS_EVENT_TIMESTAMP\",\"SHIP_TRANS_SHIP_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ff33b8-c7ba-451f-9274-704ef502cf19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### daily ship popularity (Purchased Ships Ranks -  Daily purchased ship name by amount rank)\n",
    "\n",
    "kpi_ships_daily_popularity_purchased = f_multi_ships.filter(F.col('SHIP_TRANS_SHIP_NAME') != 'null').filter(F.col('SHIP_TRANS_SC_AMOUNT') != 0).select(\"SHIP_TRANS_EVENT_TIMESTAMP\",\"SHIP_TRANS_SHIP_NAME\",\"SHIP_TRANS_SC_AMOUNT\").groupBy(\"SHIP_TRANS_EVENT_TIMESTAMP\",\"SHIP_TRANS_SHIP_NAME\").count()\n",
    "\n",
    "window_item = Window.partitionBy(kpi_ships_daily_popularity_purchased[\"SHIP_TRANS_EVENT_TIMESTAMP\"]).orderBy(desc(\"count\"))\n",
    "\n",
    "kpi_ships_daily_popularity_purchased = kpi_ships_daily_popularity_purchased.withColumn(\"purchase_rank\",dense_rank().over(window_item))\n",
    "\n",
    "##### daily ship popularity (Sold Ships Ranks -  Daily sold ship name by amount rank)\n",
    "\n",
    "kpi_ships_daily_popularity_sold = f_multi_ships.filter(F.col('SHIP_TRANS_SHIP_NAME') != 'null').filter(F.col('SHIP_TRANS_SC_AMOUNT') == 0).select(\"SHIP_TRANS_EVENT_TIMESTAMP\",\"SHIP_TRANS_SHIP_NAME\",\"SHIP_TRANS_SC_AMOUNT\").groupBy(\"SHIP_TRANS_EVENT_TIMESTAMP\",\"SHIP_TRANS_SHIP_NAME\").count()\n",
    "\n",
    "window_item = Window.partitionBy(kpi_ships_daily_popularity_sold[\"SHIP_TRANS_EVENT_TIMESTAMP\"]).orderBy(desc(\"count\"))\n",
    "\n",
    "kpi_ships_daily_popularity_sold = kpi_ships_daily_popularity_sold.withColumn(\"purchase_rank\",dense_rank().over(window_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322c1d2f-2084-43b1-ba4a-dbd1d2d26cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### User transactions overview:\n",
    " - amount of battles, logins, days since registration before first purchase\n",
    " - daily/weekly/monthly revenue per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8860f3-f340-498c-96eb-51f645860f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# amount of battles, logins, days since registration before first purchase\n",
    "\n",
    "cond = [(f_multi_ships.IN_APP_USER_ID == d_new_user.USER_USER_ID)]\n",
    "\n",
    "new_user_fact_join = f_multi_ships.join(d_new_user,on = cond, how=\"inner\")\n",
    "\n",
    "window_item = Window.partitionBy(new_user_fact_join[\"USER_USER_ID\"])\n",
    "\n",
    "new_user_fact_join = new_user_fact_join.select(\"USER_USER_ID\",\"IN_APP_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"IN_APP_EVENT_TIMESTAMP\").groupBy(\"USER_USER_ID\",\"IN_APP_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"IN_APP_EVENT_TIMESTAMP\").agg(min(col(\"IN_APP_EVENT_TIMESTAMP\")).over(window_item).alias(\"First_Purchase_Date\"))\n",
    "\n",
    "new_user_fact_join = new_user_fact_join.dropDuplicates([\"USER_USER_ID\"]).select(\"USER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"First_Purchase_Date\").withColumnRenamed(\"USER_EVENT_TIMESTAMP\",\"Registration_Date\")\n",
    "\n",
    "cond = [(f_multi_ships.SESSION_USER_ID == new_user_fact_join.USER_USER_ID)]\n",
    "\n",
    "f_multi_ships_reg_purchase_date = f_multi_ships.join(new_user_fact_join,on = cond, how=\"inner\").select(\"USER_USER_ID\",\"SESSION_SESSION_ID\",\"First_Purchase_Date\",\"Registration_Date\",\"MULTIPLAYER_BATTLE_ID\",\"SESSION_EVENT_TIMESTAMP\")\n",
    "\n",
    "f_multi_ships_reg_purchase_date = f_multi_ships_reg_purchase_date.where(f_multi_ships_reg_purchase_date.SESSION_EVENT_TIMESTAMP < f_multi_ships_reg_purchase_date.First_Purchase_Date)\n",
    "\n",
    "kpi_login_amounts_since_reg =  f_multi_ships_reg_purchase_date.select(\"USER_USER_ID\",\"SESSION_SESSION_ID\").groupBy(\"USER_USER_ID\").count()\n",
    "\n",
    "kpi_battle_amounts_since_reg =  f_multi_ships_reg_purchase_date.filter(F.isnotnull(\"MULTIPLAYER_BATTLE_ID\")).select(\"USER_USER_ID\",\"MULTIPLAYER_BATTLE_ID\").groupBy(\"USER_USER_ID\").count()\n",
    "\n",
    "timeDiff = ((unix_timestamp('First_Purchase_Date', \"yyyy-MM-dd\") - unix_timestamp('Registration_Date', \"yyyy-MM-dd\"))/86400)\n",
    "\n",
    "kpi_days_since_reg = f_multi_ships_reg_purchase_date.withColumn(\"Day_Amount_From_Registration\", timeDiff).select(\"USER_USER_ID\",\"Day_Amount_From_Registration\").dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4c33ee-73c8-461b-bd82-272af267ccca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# daily/weekly/monthly revenue per user\n",
    "\n",
    "kpi_f_multi_ships_1_day = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 2 days')).groupBy(\"SESSION_USER_ID\").agg(sum(col(\"IN_APP_USD_COST\")).alias(\"Revenue_1_day\")).orderBy(desc(\"Revenue_1_day\"))\n",
    "\n",
    "kpi_f_multi_ships_7_day = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 8 days')).groupBy(\"SESSION_USER_ID\").agg(sum(col(\"IN_APP_USD_COST\")).alias(\"Revenue_7_day\")).orderBy(desc(\"Revenue_7_day\"))\n",
    "\n",
    "kpi_f_multi_ships_30_day = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 31 days')).groupBy(\"SESSION_USER_ID\").agg(sum(col(\"IN_APP_USD_COST\")).alias(\"Revenue_30_day\")).orderBy(desc(\"Revenue_30_day\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bece996-45b5-41fe-975f-cb9f45a3fece",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Battle analysis\n",
    " - new users participation in battles on a 1/3/7/14 day since registration\n",
    " - battle participation by active users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa83faa9-d429-4b99-8c98-901f1668ac25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 day battle participation\n",
    "\n",
    "f_multi_ships_1 = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 2 days'))\n",
    "\n",
    "cond = [(f_multi_ships_1.MULTIPLAYER_USER_ID == d_new_user.USER_USER_ID)]\n",
    "\n",
    "new_user_fact_join_battle_1 = f_multi_ships_1.join(d_new_user,on = cond, how=\"inner\")\n",
    "\n",
    "window_item = Window.partitionBy(new_user_fact_join_battle_1[\"USER_USER_ID\"])\n",
    "\n",
    "new_user_fact_join_battle_1 = new_user_fact_join_battle_1.select(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").groupBy(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").agg(count(col(\"MULTIPLAYER_USER_ID\")).over(window_item).alias(\"Battle_Participation_Number\"))\n",
    "\n",
    "kpi_new_user_fact_join_battle_1 = new_user_fact_join_battle_1.select(\"USER_USER_ID\",\"Battle_Participation_Number\").dropDuplicates()\n",
    "\n",
    "# 3 day battle participation\n",
    "\n",
    "f_multi_ships_3 = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 4 days'))\n",
    "\n",
    "cond = [(f_multi_ships_3.MULTIPLAYER_USER_ID == d_new_user.USER_USER_ID)]\n",
    "\n",
    "new_user_fact_join_battle_3 = f_multi_ships_3.join(d_new_user,on = cond, how=\"inner\")\n",
    "\n",
    "window_item = Window.partitionBy(new_user_fact_join_battle_3[\"USER_USER_ID\"])\n",
    "\n",
    "new_user_fact_join_battle_3 = new_user_fact_join_battle_3.select(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").groupBy(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").agg(count(col(\"MULTIPLAYER_USER_ID\")).over(window_item).alias(\"Battle_Participation_Number\"))\n",
    "\n",
    "kpi_new_user_fact_join_battle_3 = new_user_fact_join_battle_3.select(\"USER_USER_ID\",\"Battle_Participation_Number\").dropDuplicates()\n",
    "\n",
    "\n",
    "# 7 day battle participation\n",
    "\n",
    "f_multi_ships_7 = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 8 days'))\n",
    "\n",
    "cond = [(f_multi_ships_7.MULTIPLAYER_USER_ID == d_new_user.USER_USER_ID)]\n",
    "\n",
    "new_user_fact_join_battle_7 = f_multi_ships_7.join(d_new_user,on = cond, how=\"inner\")\n",
    "\n",
    "window_item = Window.partitionBy(new_user_fact_join_battle_7[\"USER_USER_ID\"])\n",
    "\n",
    "new_user_fact_join_battle_7 = new_user_fact_join_battle_7.select(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").groupBy(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").agg(count(col(\"MULTIPLAYER_USER_ID\")).over(window_item).alias(\"Battle_Participation_Number\"))\n",
    "\n",
    "kpi_new_user_fact_join_battle_7 = new_user_fact_join_battle_7.select(\"USER_USER_ID\",\"Battle_Participation_Number\").dropDuplicates()\n",
    "\n",
    "# 14 day battle participation\n",
    "\n",
    "f_multi_ships_14 = f_multi_ships.filter(F.col('SESSION_EVENT_TIMESTAMP') > F.expr(f'current_date() - interval 15 days'))\n",
    "\n",
    "cond = [(f_multi_ships_14.MULTIPLAYER_USER_ID == d_new_user.USER_USER_ID)]\n",
    "\n",
    "new_user_fact_join_battle_14 = f_multi_ships_14.join(d_new_user,on = cond, how=\"inner\")\n",
    "\n",
    "window_item = Window.partitionBy(new_user_fact_join_battle_14[\"USER_USER_ID\"])\n",
    "\n",
    "new_user_fact_join_battle_14 = new_user_fact_join_battle_14.select(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").groupBy(\"USER_USER_ID\",\"MULTIPLAYER_USER_ID\",\"USER_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").agg(count(col(\"MULTIPLAYER_USER_ID\")).over(window_item).alias(\"Battle_Participation_Number\"))\n",
    "\n",
    "kpi_new_user_fact_join_battle_14 = new_user_fact_join_battle_14.select(\"USER_USER_ID\",\"Battle_Participation_Number\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5267fb26-218d-4ba3-9487-868555e0d6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# battle participation by active users\n",
    "\n",
    "window_item = Window.partitionBy(f_multi_ships[\"SESSION_USER_ID\"])\n",
    "\n",
    "f_multi_ships = f_multi_ships.select(\"SESSION_USER_ID\",\"MULTIPLAYER_USER_ID\",\"SESSION_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").groupBy(\"SESSION_USER_ID\",\"MULTIPLAYER_USER_ID\",\"SESSION_EVENT_TIMESTAMP\",\"MULTIPLAYER_EVENT_TIMESTAMP\").agg(count(col(\"MULTIPLAYER_USER_ID\")).over(window_item).alias(\"Battle_Participation_Number\"))\n",
    "\n",
    "kpi_active_user_join_battle = f_multi_ships.select(\"SESSION_USER_ID\",\"Battle_Participation_Number\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3502a82c-6d92-49c2-9c22-7b68587fb01d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Writing KPIs tables to third layer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d698efee-fed1-4e45-96ef-b8daaae97a7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kpi_active_user_count\nTable exists on hive_metastore.third_layer_pipeline.kpi_active_user_count_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_active_user_count_golden_layer_managed_table\nkpi_revenue_sum\nTable exists on hive_metastore.third_layer_pipeline.kpi_revenue_sum_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_revenue_sum_golden_layer_managed_table\nkpi_arpu_df\nTable exists on hive_metastore.third_layer_pipeline.kpi_arpu_df_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_arpu_df_golden_layer_managed_table\nkpi_new_user_count\nTable exists on hive_metastore.third_layer_pipeline.kpi_new_user_count_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_new_user_count_golden_layer_managed_table\nkpi_spenders_count\nTable exists on hive_metastore.third_layer_pipeline.kpi_spenders_count_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_spenders_count_golden_layer_managed_table\nkpi_arrpu_df\nTable exists on hive_metastore.third_layer_pipeline.kpi_arrpu_df_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_arrpu_df_golden_layer_managed_table\nkpi_one_day_ret_rate\nTable exists on hive_metastore.third_layer_pipeline.kpi_one_day_ret_rate_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_one_day_ret_rate_golden_layer_managed_table\nkpi_three_day_ret_rate\nTable exists on hive_metastore.third_layer_pipeline.kpi_three_day_ret_rate_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_three_day_ret_rate_golden_layer_managed_table\nkpi_seven_day_ret_rate\nTable exists on hive_metastore.third_layer_pipeline.kpi_seven_day_ret_rate_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_seven_day_ret_rate_golden_layer_managed_table\nkpi_seven_day_conv_rate\nTable exists on hive_metastore.third_layer_pipeline.kpi_seven_day_conv_rate_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_seven_day_conv_rate_golden_layer_managed_table\nkpi_ships_popularity\nTable exists on hive_metastore.third_layer_pipeline.kpi_ships_popularity_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_ships_popularity_golden_layer_managed_table\nkpi_ships_daily_popularity_purchased\nTable exists on hive_metastore.third_layer_pipeline.kpi_ships_daily_popularity_purchased_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_ships_daily_popularity_purchased_golden_layer_managed_table\nkpi_ships_daily_popularity_sold\nTable exists on hive_metastore.third_layer_pipeline.kpi_ships_daily_popularity_sold_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_ships_daily_popularity_sold_golden_layer_managed_table\nkpi_login_amounts_since_reg\nTable exists on hive_metastore.third_layer_pipeline.kpi_login_amounts_since_reg_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_login_amounts_since_reg_golden_layer_managed_table\nkpi_battle_amounts_since_reg\nTable exists on hive_metastore.third_layer_pipeline.kpi_battle_amounts_since_reg_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_battle_amounts_since_reg_golden_layer_managed_table\nkpi_days_since_reg\nTable exists on hive_metastore.third_layer_pipeline.kpi_days_since_reg_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_days_since_reg_golden_layer_managed_table\nkpi_f_multi_ships_1_day\nTable exists on hive_metastore.third_layer_pipeline.kpi_f_multi_ships_1_day_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_f_multi_ships_1_day_golden_layer_managed_table\nkpi_f_multi_ships_7_day\nTable exists on hive_metastore.third_layer_pipeline.kpi_f_multi_ships_7_day_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_f_multi_ships_7_day_golden_layer_managed_table\nkpi_f_multi_ships_30_day\nTable exists on hive_metastore.third_layer_pipeline.kpi_f_multi_ships_30_day_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_f_multi_ships_30_day_golden_layer_managed_table\nkpi_new_user_fact_join_battle_1\nTable exists on hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_1_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_1_golden_layer_managed_table\nkpi_new_user_fact_join_battle_3\nTable exists on hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_3_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_3_golden_layer_managed_table\nkpi_new_user_fact_join_battle_7\nTable exists on hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_7_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_7_golden_layer_managed_table\nkpi_new_user_fact_join_battle_14\nTable exists on hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_14_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_new_user_fact_join_battle_14_golden_layer_managed_table\nkpi_active_user_join_battle\nTable exists on hive_metastore.third_layer_pipeline.kpi_active_user_join_battle_golden_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.third_layer_pipeline.kpi_active_user_join_battle_golden_layer_managed_table\n"
     ]
    }
   ],
   "source": [
    "def list_dataframes():\n",
    "    \"\"\"\n",
    "    Lists of dataframes names as list.\n",
    "\n",
    "    Returns:\n",
    "        list: list consist of names.\n",
    "    \"\"\"\n",
    "    df_names = [k for (k, v) in globals().items() if isinstance(v, DataFrame)]\n",
    "\n",
    "    return df_names\n",
    "\n",
    "for df_name in list_dataframes():\n",
    "    if \"kpi\" in df_name:\n",
    "        print(df_name)\n",
    "    # Writing kpi dataframes to 3rd layer on defined schema and table\n",
    "        write_to_managed_table(globals() [df_name], df_name,  third_layer_schema_name, location_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa3d0d4-6a3a-4aed-b427-1d2b9400857d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### KPIS can be addded to workflows as Live Table (Materialized View) some examples can be found below, however this method is not being used for this task`s solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219fcb01-23ec-40e6-8d36-ef82e611c732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Active Users\n",
    "\n",
    "# @dlt.table\n",
    "# def f_multi_ships_fact_table_read():\n",
    "#   return spark.table(\"hive_metastore.third_layer_pipeline.f_multi_ships_golden_layer_managed_table\")\n",
    "\n",
    "\n",
    "# @dlt.table(\n",
    "#   comment=\"A table containing active users count.\"\n",
    "# )\n",
    "# def active_users_count():\n",
    "#   return (\n",
    "#     dlt.read('f_multi_ships_fact_table_read')\n",
    "#     .agg(countDistinct(col(\"SESSION_USER_ID\")).alias(\"Active_User_Number\"))\n",
    "#   )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a527f0a9-819a-4487-8cc8-8d06b49ef7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New Users\n",
    "\n",
    "# @dlt.table\n",
    "# def d_new_users_table_read():\n",
    "#   return spark.table(\"hive_metastore.third_layer_pipeline.d_new_user_golden_layer_managed_table\")\n",
    "\n",
    "# @dlt.table(\n",
    "#   comment=\"A table containing new users count.\"\n",
    "# )\n",
    "# def new_users_count():\n",
    "#   return (\n",
    "#     dlt.read('d_new_users_table_read')\n",
    "#     .agg(countDistinct(col(\"USER_USER_ID\")).alias(\"New_User_Number\"))\n",
    "#   )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Golden_Layer_Notebook",
   "widgets": {
    "day_parameter": {
     "currentValue": "15",
     "nuid": "c5a3b56a-a5f4-4f15-97e9-d1bc4f45325c",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "2",
      "label": null,
      "name": "day_parameter",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "2",
        "8",
        "15",
        "31"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
