{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa5cf6d-1b2c-48c5-9454-790760d673c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# This Notebook is Made to Create Bronze Layer Tables (Ingestion Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ba742d-2182-45ef-bb79-3ef9dd8a031d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Packages and Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fba833-3a19-4039-8868-834e6058eb7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: faker in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ceb7c425-be86-4b3f-8fd4-04bdb4416734/lib/python3.10/site-packages (21.0.0)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.10/site-packages (from faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b058b0-c2d3-43e2-a832-1e729e45affb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "# import pyspark as spark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import countDistinct,md5,concat,when,col\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,StringType,VarcharType,LongType\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "import csv \n",
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afb1452-f183-43b8-ac52-82b5d182de27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting Variables and Dataframes from config notebook\n",
    "%run /Workspace/Users/cihangiray.oner@gmail.com/congif.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df14b12f-28b5-48e5-ac07-be795edae0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating raw data holding DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a232a2f-a9f9-474b-a6d4-20721698fb1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from faker.providers import DynamicProvider\n",
    "def create_dataframe(table_name,size=1000):\n",
    "    '''\n",
    "    By this function a fake value holding table being created to replicate a table\n",
    "    that reflects given table structures by task documents.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    fake = Faker()\n",
    "                \n",
    "    #Unique ID dataset provider\n",
    "    UID_provider = DynamicProvider(\n",
    "    provider_name=\"UID_provider\",\n",
    "    elements=[*range(1, 1100, 1)]\n",
    "    )\n",
    "\n",
    "    fake.add_provider(UID_provider)\n",
    "    \n",
    "    #Random ID dataset provider\n",
    "    ID_provider = DynamicProvider(\n",
    "        provider_name=\"ID_provider\",\n",
    "        elements=[*range(0, 10, 1)]\n",
    "    )\n",
    "\n",
    "    fake.add_provider(ID_provider)\n",
    "\n",
    "    \n",
    "    \n",
    "    #ship_name dataset provider\n",
    "    ship_name_provider = DynamicProvider(\n",
    "        provider_name=\"ship_name_provider\",\n",
    "        elements=['frigates','frigates_1','frigates_2','frigates_3','frigates_4']\n",
    "    )\n",
    "\n",
    "    fake.add_provider(ship_name_provider)\n",
    "    \n",
    "    # Apeend values into a dict by conditions over column names or data types\n",
    "    col_dataframe_names = {}\n",
    "\n",
    "    for col_name,data_type in table_name.items():\n",
    "        if \"TIMESTAMP\" in data_type:\n",
    "            col_dataframe_names[col_name] = [fake.date_this_month() for time_stamp in range(size)]\n",
    "        elif \"SESSION_ID\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.unique.UID_provider() for ID in range(size)]\n",
    "        elif \"USER_ID\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.ID_provider() for ID in range(size)]\n",
    "        elif \"SHIP_NAME\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.ship_name_provider() for ID in range(size)]\n",
    "        elif \"BOOLEAN\" in data_type or \"IS\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.boolean() for ID in range(size)]\n",
    "        elif \"NUMBER\" in data_type or \"ID\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.ID_provider() for ID in range(size)]\n",
    "        elif \"COUNTRY\" in col_name:\n",
    "            col_dataframe_names[col_name] = [fake.country() for ID in range(size)]\n",
    "        else:\n",
    "            col_dataframe_names[col_name] = [fake.color_name() for ID in range(size)]\n",
    "    # Geting Column Names\n",
    "    columns = list(col_dataframe_names.keys())\n",
    "    # Getting Columns with Values\n",
    "    data = [[*vals] for vals in zip(*col_dataframe_names.values())]\n",
    "    # Putting Into a DataFrame\n",
    "    df_1 = spark.createDataFrame(data, columns)\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4209a52-8185-4342-887c-53cc1511de0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Concat Columns Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eae576e-1415-45e5-9a29-39cb69abf5be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concat_columns(dataframe_name,concat_columns,col_name):\n",
    "       \"\"\"\n",
    "       Concats specified columns into one.\n",
    "\n",
    "       Args:\n",
    "        dataframe_name (pyspark.sql.DataFrame): The DataFrame to concatenate.\n",
    "        concat_columns (list): The column name to concatenate within each group.\n",
    "        col_name (str): Column name that holds concat values\n",
    "\n",
    "        Returns:\n",
    "        dataframe_name: The DataFrame after concat the columns.\n",
    "\n",
    "       Example:\n",
    "        df\n",
    "        +---+------+\n",
    "        | id| label|\n",
    "        +---+------+\n",
    "        |  1|Label1|\n",
    "        |  2|Label2|\n",
    "        +---+------+\n",
    "        concat_columns = [\"col1\",\"col2\"]\n",
    "        \n",
    "        Result:\n",
    "        +---+---------------+\n",
    "        | id|col_name       |\n",
    "        +---+---------------+\n",
    "        |  1|1Label1        |\n",
    "        |  2|2Label2        |\n",
    "        +---+---------------+\n",
    "       \"\"\"\n",
    "       dataframe_name = dataframe_name.withColumn(col_name,(concat(*concat_columns)))\n",
    "       return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a947fd-5a78-4d89-a2ec-34fe33719763",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Writing Dataframes into Hive Metastore Bronze Layer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2faa18b2-f6a8-4f76-9710-de8825662be3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_schema(first_layer_schema_name,location_name):\n",
    "    \"\"\"\n",
    "    Checks if a schema exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        location_name (str): catalog name to save schema on\n",
    "        first_layer_schema_name (str): The name of the schema to check.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DF creates schema on defined catalog if not exits\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"CREATE SCHEMA IF NOT EXISTS  {location_name}.{first_layer_schema_name}\")\n",
    "\n",
    "\n",
    "def check_if_table_exists(first_layer_schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Checks if a table exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return spark.catalog.tableExists(f\"hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "\n",
    "def write_to_managed_table(df, table_name, first_layer_schema_name, location_name, mode = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a managed table in Delta Lake.\n",
    "\n",
    "    If the table exists and mode is overwrite, it performs an overwrite operation.\n",
    "    Otherwise, it either creates a new table or appends transactions to table based on the `mode` parameter.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The DataFrame to write to the table.\n",
    "        table_name (str): The name of the target table.\n",
    "        first_layer_schema_name (str): The schema name of target table.\n",
    "        location_name (str): Catalog name to save schema on\n",
    "        mode (str, optional): The write mode.\n",
    "    \"\"\"\n",
    "    #create schema if not exists\n",
    "    create_schema(first_layer_schema_name,location_name)\n",
    "    # check if the table exists\n",
    "    if check_if_table_exists(first_layer_schema_name, table_name):\n",
    "        print(f\"Table exists on hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        if mode == \"overwrite\":\n",
    "            print(f\"Overwriting all transactions on managed table hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "            df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        else:\n",
    "            print(f\"Appending all transactions on managed table hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "            df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "    else:\n",
    "        print(f\"Writing to managed table hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").saveAsTable(f\"hive_metastore.{first_layer_schema_name}.{table_name}_bronze_layer_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b39175-2286-4a44-b2be-80eeec3fc040",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key,value in table_names_list.items():\n",
    "    print(key)\n",
    "    # Creating dataframes by create_dataframe function\n",
    "    vars() [key] = create_dataframe(value)\n",
    "    vars() [key] = concat_columns(vars() [key],[\"SESSION_ID\",\"USER_ID\"],\"Join_key\")\n",
    "    # Writing dataframes to 1st layer on defined schema and table\n",
    "    write_to_managed_table(vars()[key], key,  first_layer_schema_name, location_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer_Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
